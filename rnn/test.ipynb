{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from datetime import datetime\n",
    "from datetime import timedelta\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import sys\n",
    "import itertools\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import random\n",
    "import os\n",
    "import pickle\n",
    "import codecs\n",
    "from gensim import corpora\n",
    "import gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load data from files\n",
    "with open('../data/processed_trumptwitterarchive.txt', 'r') as data_file:\n",
    "    json_data = data_file.read()\n",
    "data = json.loads(json_data)\n",
    "#load label from files\n",
    "labels = pd.read_csv('../data/label.csv', names =['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(23522, 300)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [sample['processed_text'] for sample in data if len(sample['processed_text'])!=0]\n",
    "dictionary = corpora.Dictionary(texts) \n",
    "word2id_dict = dictionary.token2id\n",
    "np.random.uniform(-0.25, 0.25, (len(word2id_dict), 300)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.uniform?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self):\n",
    "        # the prcessed words for each tweet: delete none processed_text data\n",
    "        self.texts = [sample['processed_text'] for sample in data if len(sample['processed_text'])!=0]\n",
    "        self.retweet_count = [sample['retweet_count'] for sample in data if len(sample['processed_text'])!=0]\n",
    "        self.favorite_count = [sample['favorite_count'] for sample in data if len(sample['processed_text'])!=0]\n",
    "\n",
    "        #word dictionary\n",
    "        dictionary = corpora.Dictionary(self.texts) \n",
    "        self.word2id_dict = dictionary.token2id  # transform to dict, like {\"human\":0, \"a\":1,...}\n",
    "\n",
    "        #set lables from csv file: up is 1; down is 0\n",
    "        self.lables = labels['label'].tolist()\n",
    "        # delete\n",
    "        examples_lables = [x for x in zip(self.texts, self.lables) if len(x[0])!= 0]\n",
    "        random.shuffle(examples_lables)\n",
    "        self.MyDataset_frame = examples_lables\n",
    "\n",
    "        #transform word to id\n",
    "        self.MyDataset_wordid = \\\n",
    "            [(\n",
    "                np.array([self.word2id_dict[word] for word in sent[0]], dtype=np.int64), \n",
    "                sent[1]\n",
    "            ) for sent in self.MyDataset_frame]\n",
    "        \n",
    "    def word_embeddings(self, path = './GoogleNews-vectors-negative300.bin/GoogleNews-vectors-negative300.bin'):\n",
    "\t    #establish from google\n",
    "\t    model = gensim.models.KeyedVectors.load_word2vec_format(path, binary=True)\n",
    "\t    print('Please wait ... (it could take a while to load the file : {})'.format(path))\n",
    "\n",
    "\t    word_dict = self.word2id_dict\n",
    "\t    embedding_weights = np.random.uniform(-0.25, 0.25, (len(self.word2id_dict), 300))\n",
    "\n",
    "\t    for word in word_dict:\n",
    "            word_id = word_dict[word]\n",
    "            if word in model.wv.vocab:\n",
    "                embedding_weights[word_id, :] = model[word]\n",
    "\n",
    "\t    return embedding_weights\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.MyDataset_frame)\n",
    "\n",
    "    def __getitem__(self,idx):\n",
    "        sample = self.MyDataset_wordid[idx]      \n",
    "        return sample\n",
    "\n",
    "    def getsent(self, idx):\n",
    "        sample = self.MyDataset_wordid[idx][0]       \n",
    "        return sample\n",
    "\n",
    "    def getlabel(self, idx):\n",
    "        label = self.MRDataset_wordid[idx][1]\n",
    "        return label\n",
    "\n",
    "\n",
    "    def word2id(self):\n",
    "        return self.word2id_dict\n",
    "\n",
    "    def id2word(self):\n",
    "        id2word_dict = dict([val,key] for key,val in self.word2id_dict.items()) \n",
    "        return id2word_dict\n",
    "    \n",
    "\n",
    "class train_set(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.train_frame = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.train_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.train_frame[idx]\n",
    "\n",
    "\n",
    "class test_set(Dataset):\n",
    "    def __init__(self, samples):\n",
    "        self.test_frame = samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.test_frame)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.test_frame[idx]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
